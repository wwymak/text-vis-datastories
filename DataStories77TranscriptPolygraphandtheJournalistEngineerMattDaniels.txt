Data Stories #77  |   Transcript
Polygraph and the Journalist Engineer Matt Daniels
http://datastori.es/77-polygraph-and-the-journalist-engineer-matt-daniels/


Episode Index 
* Our sponsor: CartoDB
* Moritz and Enrico catch up
* Introducing Matt Daniels from Polygraph
* The origins of Polygraph
* The Most Timeless Songs of All Time project
* Matt’s visual design
* Spotify’s API
* The process for creating a project like The Most Timeless Songs of All Time
* Our sponsor: CartoDB
* Matt’s project on The Bechdel Test
* The follow-up project: Film Dialogue from 2,000 Screenplays, Broken Down by Gender and Age
* The financials behind Matt’s work
* Stay tuned on Polygraph: Matt’s next project
* Interested in collaborating with Matt? Get in touch
* Rate us and get in touch!
* Our sponsor: CartoDB


"I don't know what I'm trying to visualize, so instead of trying to visualize the insight, I'm actually trying to visualize the data and the story."


00:11 Our sponsor: CartoDB
  

This episode is sponsored by CartoDB. CartoDB is an open, powerful, and intuitive platform for discovering and predicting the key facts facts underlying the massive location data in our world. With CartoDB, you can design and analyze beautiful and insightful maps. Check out incredible location intelligence projects and get started for free at cartodb.com/gallery.






00:44 Moritz and Enrico catch up
  
                
Enrico Bertini: Hey, everyone. Welcome to a new episodes of Data Stories. Hey, Moritz. What's up?


Moritz Stefaner: Hey, Enrico. Um, it's summertime and I'm preparing my trip to the U.S., so we're touring the U.S. in three weeks, starting on Sunday. I'm excited, yeah.


EB: You are touring. 


MS: Yeah. And you know this thing when you have a big presentation to give, and then, um, you really want to nail it, and then you start rewriting old Flash-based web applications instead?


EB: Yep. 


MS: That's what I did this week, so good for me. My presentation should be fine, too, yeah. I just had to redo the Twitter visualization I did the last... (inaudible)


EB: Oh, I loved that. I loved that one. 


MS: I had to rebuild it. What could I do? So.


EB: Oh, yeah. Yeah, yeah, yeah. So, when are you going? You're going to IO? 


MS: Uh, Minneapolis…


EB: Minneapolis, yeah.


MS: Vancouver, San Francisco, and Boston. 


EB: Nice, nice. 


MS: How about you? What are you up to? 


EB: Um, good. Enjoying summer a little bit. Still a lot of work to do, but it's fine. I'm done with teaching, which is good. Um, I just have a brief update. I'm really happy about a recent, uh, project. Um, I don't know if I've ever mentioned that on the podcast, but we are working with ProPublica and developing some software to help them look into millions of reviews from, from Yelp. They just published their second article based on the analysis that they conducted with this tool, and I'm really excited. Some of the stuff that we do is actually useful, at least to some people within ProPublica. So they've been analyzing Yelp reviews to actually look into privacy issues, and the fact that doctors sometime reply to reviews, customer reviews, and disclose information that they are not allowed to, to disclose. So, it's new stuff. It's fun, yeah, it's fun. 


MS: So, you will put the article in the (inaudible), yeah? 


EB: I will, of course. 


MS: Absolutely, very nice. So I'll read it. 




02:47 Introducing Matt Daniels from Polygraph 
  

EB: So, let's start with our new guest today. I'm really excited to have Matt Daniels on the show, from Polygraph. Hey, Matt. How are you?


Matt Daniels: I'm great. How are you? 


EB: I'm good, I'm good. So, uh, Matt is the, is the main person behind Polygraph, and, um, he publishes amazing, um, how do you call that? I mean, interactive articles with visualizations and data analysis. Mostly a lot of them on music, and that's one of the reason why I'm so excited, but also many other topics, including, I don't know, gender biases, and stuff like that. So, Matt, can you give us a little bit of an introduction about who you are, what you do, um, maybe even why you do it? And then we can move onto a couple of projects we want to talk about. 




03:41 The origins of Polygraph
  

MD: Sure. Yeah, so I started coding really heavily about a year and a half ago, and, uh, before then I'd always published, you know, weird things on the internet. Uh, I always just had side projects on top of my full-time job. And around February of 2015, I stopped doing them as side projects and started doing them full-time. So, uh, invested heavily in just learning how to code, and instead of just doing these small side projects, spending my waking hours just doing one project at a time, rather than dividing my time between a full-time gig and, and hobbies, uh, nights and weekends. And then, yeah, so, I had been doing these side projects and publishing them on my personal site, essential like a mattdaniels.com, uh, and then about six or seven months ago, uh, I started another site, uh, so that it could, you know, grow bigger than myself, and have a little bit more serendipity in terms of how the, the stories could be larger than, than the individual pieces, uh, and turn to just a broader idea. Um, so I just registered domain Polygraph, and then have been making things ever since. 


MS: So, um, let me just ask you this. So, this means that you started coding, what, one year ago or so?


MD: Uh, I did my first real Javascript project, uh, roughly February of 2015, yeah.


EB: Wow. I mean, when I look at what you do on the web, it's amazing. Congratulations. 


MD: Wow, thank you. 


EB: Doesn't take long to learn coding. 


MD: No, no. 


MS: It's so easy. 


EB: No, because, you know, I mean, I'm saying that because you have those people who are, like, do I need to learn coding in order to do visualization? Well, not necessarily, but if you do, I mean, you have so many more options, so. I think it's, you are a good story to tell. So, Matt, I would like to dive into, um, a couple of projects we selected, um, directly. Uh, you have many, many more, and I really encourage our listeners to just go to your website, which is poly-graph.co and see all the amazing projects you publish there. So, we will be focusing on a couple of them. 




06:13 The Most Timeless Songs of All Time project
  

EB: And, uh, I would like to start with one about music, and it's called The Most Timeless Songs of All Time. So, can you briefly introduce this project and tell us how you, you, you got started there? Uh, how you generated the idea, and then how you realized the project? 


MD: Yeah, this was actually the first project on Polygraph. And by the way, we've upgraded the domain to polygraph.cool. No hyphens and co's. It's way easier to remember, um, in, like, water cooler conversation. So, uh, yeah, so the Timeless project was actually the impetus for, for the site. It was the first project that I decided not to put on my personal blog, and, and, and make it on this new thing. And the, the story behind that project was, and this is, like, about nine months ago, um, September of 2015, is I was walking down the street listening to, I think, "Back That Thing Up," -- uh, let's keep it safe for work, this podcast, by Juvenile -- and I was, like, this is a good song, but I wonder if, uh, like, my children's children will, like, hear this song and think about it as fondly as I think about songs from the '50s, like by Frank Sinatra or Etta James. Will they, like, think it's really absurd that, like, this was a thing in the early 2000s? Um, so from there, I, I was like, man, it would be really interesting to see what's still popular today from maybe 10 or 15, 20 years ago, as a way to start predicting what is standing the test of time. And will there be an instance where, in 2030 or 2040, people will look at our music, and it will be just as much in the zeitgeist as, like, a Frank Sinatra is today? So, uh, yeah, that was kind of the spirit of the project, and then the way it manifested into the article was getting Spotify data for a full year. Uh, we used 2014 data, since this was published in 2015, late 2015, and looked at what is the most popular music today from the '90s, the '80s, the '70s, '60s, et cetera, on Spotify? Which was a really interesting measure when you consider that, alright, music from the '50s is now, uh, 65 years old. Like, we probably are at a point where it's reached a stable as it, like, equilibrium where, yes, our children, our children's children will probably listen to the same '60s music as we did. Um, so, yeah, so we had a really, we had a lot of interesting trend data, um, to work off of. And, yeah, that was the spirit of the project. So, you can go to the site, and look at what is the most popular song from the '90s, and also see where, uh, "Back That Thing Up" stands.




09:00 Matt's visual design
  

EB: So, I'm wondering if we can try to describe a little bit how this little page looks like. Um, I'll try myself to say something about it. So, I think, and please correct me if I'm wrong, most of your projects have a similar style. You start with a big title, some text, then you have some interactive charts, then more text, then more interactive charts. Something I really, really like. And, um, and you talk about the data analysis behind it, how you collected the data, and provide quite a few details. And then you do the analysis itself, and describe what this means. So, in this case, you have one chart, where there are, like, um, songs, um, placed in a graph, and you can see how many songs from the '90s, right? It's called "What's Remembered from the 90s," and you have each song represented by one, one dot, with the face of the singer, main singer, behind it. And you can see on the far right, there is Kurt Cobain with the most popular song ever, with, what? Fifty million, more than 50 millions plays. And, um, yeah, and you have many others on the left. Um, yeah, it's, it's very interesting, and I think what I really like of the analysis that you made, and the, and how you comment on this, is that you, you, especially on this specific piece, you've been commenting on the idea that some, there is a difference between, um, songs that are still popular today after many years, and how they scored back then in, uh, in Billboard and similar charts, right? And not necessarily the most popular song back then are those that, um, are still popular today. 


MD: Yeah, that is, I think that's the most interesting part from a story perspective, um, really ignoring, like, the visuals for a second, is that there's definitely a tension between what's, like, past popularity not necessarily correlating highly with present-day popularity, and the implication of that is you can take a, you know, "Single Ladies" by Beyonce, or a Taylor Swift song, and say, okay, that's so culturally pervasive, we can't imagine a world where your grandchildren would not know that song. Um, but there are also plenty of examples from the '50s and '60s and '70s where you had effectively the "Single Ladies" of their day, uh, you know, the number one songs that were so pervasive and charted at number one for so long, that you couldn't imagine a world where people in 2016 would not be listening to it, but there are actually plenty of examples. Then, actually, the inverse is also true. You have arguably, like, underground songs, kind of like a Lana del Rey song from today, that surely is popular, but isn't Taylor Swift-Beyonce popular. And they have actually grown significantly in popularity over time, far outpacing their, um, the popularity that they had when they were released. So, a good example of this is, uh, Etta James' "At Last," which actually charted on Billboard; however, did not chart very well, you know, in, not in the top 10, just for, like, a week or so, barely registered, and has, for some reason or another, slowly grown in popularity over time. And, I think, it's now, like, a popular wedding song. There's lots of reasons why it's played so often, uh, I think it was played at one of U.S. President Obama's, um, maybe it was his inauguration. Either way, it's used very often, it's in a lot of soundtracks, it's in a lot of samples. We could argue about why it's popular today, but regardless, it has gotten more popular. So those are the types of interesting trends to find in the charts, and because they're coded, you can look for the songs and look for the trends and see things that I would have never been able to see on my own. 




13:12 Spotify's API
  

MS: And you used the Spotify API for that, for the current plays, right? Is that an API that gives you a lot of access to, like, the low-level data, or did you have to, uh, do a lot of tricks to, to extract the relevant information? How is it working with that data source?


MD: Uh, Spotify only releases lifetime plays for the top 10 songs for each artist. So, uh, this was done via a private data dump from one of their data partners who is now owned by a different company, so they kind of severed ties. However, uh, I had access to the data via them, and then they went, and then once the project was finished, uh, we had to go to Spotify and say, hey, we made this thing.


MS: Okay, yeah, yeah, yeah.


MD: With your data. Are you cool with us publishing on the internet? And they could be, like, absolutely not. Uh, shut it down. Or they would be, like, this is great. So, they fortunately said this is great, and, uh, yeah, we went public with it.


MS: In the end, it's a great advertisement for them, so I think they should have paid you some money.


MD: I know, right? Yeah, it's okay. 


MS: You got to use the data, so that's already good. Yeah. And, but, you're saying the top 10 songs per artist are available nevertheless, so you could do something similar, but a bit more, like, not as complete, yeah?


MD: Um, yeah, it's actually not available via the API, so you'd have to go into the app…


MS: Go into the artist pages, and scrape everything, and yeah.


MD: Hey, that was my, that was V1 for me. I spent, like, a full day just "scraping," air quotes, uh, going to the app and typing in the plays into a spreadsheet, so once I did that, I had a good idea that the data would be interesting, um, and then, from there, actually, went to the, uh, data partner and got the real data, which was, which is actually a mess. It was a very complicated process. It wasn't easy as, like, oh, here's the data. Um, believe it or not, like, getting plays out of, getting plays for a song like "At Last" is a very complex thing. Um, and then, uh, the API for Spotify actually has, uh, popularity data, but it's indexed to 100, so you can actually do something very similar, it just won't be hard view counts for, uh, or play counts for the songs. 




15:30 The process for creating a project like The Most Timeless Songs of All Time
  

EB: Yeah, so, can you, can you briefly describe how the process works, for instance, for this project, right? So, I guess, you start from, from a question, and, uh, then you go about trying to see if you have data to answer this question. And, um, and how do you decide on, uh, um, what visualizations to use, how to design the page itself, what is the narrative structure? I mean, it looks to me that you are, uh, playing a lot with different ways of, um, giving a structure to your story. Um, so how do you do that? 


MD: Yeah, a lot has changed over the past nine months in terms of designing these articles. With the one that we're talking about, generally I try to avoid any initial analysis. Like, I have an idea of what the data looks like, but I don't really know what it says until I make the chart, um, which is a little bit counterintuitive. Um, I think most people from a data visualization perspective will do the sequel queries, uh, do the analysis in Excel, run the models, run the regressions, and then have an answer, and then try to visualize the analysis and say, okay, here's the thing that I think is interesting in the data. Um, so they have an insight and they try to represent that insight, uh, via some chart. So, I don't do any of that, and it's very problematic in many ways, because I don't know what I'm trying to visualize, so instead of trying to visualize the insight, I'm actually trying to visualize the data and the story. Um, so I avoid any sequel queries or Excel analysis. I don't actually know what the data says until I actually see the D3 visualization in the browser. Um, so, for example, if I wanted to visualize the, uh, the top '90s songs, this chart specifically was very weird, but I didn't know what would be number one. I mean, I had an idea. I peeked, but I didn't know what would be number two and number three until the chart was made. Uh, and I was, like, oh, this is, this is interesting. Look how far number one is from number three. Um, or I would make the chart and it would be, like, maybe a boring table, and I'd be, like, oh, here's what's number one and number three, but it's really not that, I don't see anything interesting in this, so I would try another chart until I really see something interesting, um, in terms of how, uh, what the variance is among, among the top 50, which is really high for the '90s songs on Spotify. So, really, it's just keep, keep trying different visualizations until you see answers to your initial question, which was, in this case, what is still popular, um, today from past decades?


MS: And will you write the text and do the sequencing of the charts afterwards, basically, when you have a good idea of what, what seems to be interesting, and what seems to be a good way to present the topic? 


MD: Yeah. The first chart is really the answer to the question, 'cause you expect nobody's gonna scroll past the first chart, uh, which is generally always true. Uh, and then the narrative structure is, is more of a necessity. It's a burden, in my mind. Um, I've actually, and this is a very divisive, um, thing that I do as well, is I'm trying to kill all prose in my work. Um, which is weird, because you need prose to explain the story. Um, but actually, I think it's a little bit of a crutch. It means that I, I need the prose to explain what the visuals say, rather than the visuals to explain what the visuals say. 


MS: Yeah, but you can also give background, or talk about causality behind, you know, the plain surface information that everybody sees, right? I mean, I think that can be quite valuable. I mean, it's a bit pointless to say, like, and as we can see, blah is number one. You know? That's, like, yeah, duh. But, yeah.


MD: Yeah, um, but you're right, like, it's, in the, in the narrative, I went through exactly what I've talked with you all, which is you have songs that were effectively the "Single Ladies" of the '50s, that aren't popular today, and it's really hard to say in a chart, uh, and really easy to say in prose; however, uh, I am trying to get to a point where I can carry that story with just visuals and as little sentences as possible. Um, from a work standpoint, it is making that first chart, making sure it answers that initial question that I had. And then, uh, fleshing out the nuances in the question, such as that disparity between historic and present-day popularity in further charts. And then obviously some prose to connect everything together. 


EB: But I have to say, Matt, my personal experience reading, using your projects is that I really like the text part that you produce. And especially the sequence, right? So, I, I, for instance, like the fact that you start from a clear question, and it's an interesting question, then you go about trying to answer that specific one, and then you make it broader, right? And, um, and another thing I like is that you first try to list the facts, what you can read out of the chart, but towards the end of your article, you kind of try to see, to generate hypotheses about, um, what phenomenon is behind, or what causality exists behind the, the facts that you extracted out of data. Um, I don't know if you do this on purpose or just you happen to do it this way, but I find it really, really interesting. So, for instance, just to give you an example, in the same piece we are talking about, "Timeless Songs," I think towards the end of your article you talk about, um, why does this happen, right? And you come up with hypotheses about why does this happen, that some songs are popular when they are published and but still they don't, um, they are no longer popular after 10 or 20 years. And, yeah, I found this really interesting. 


MD: Yeah, that, that, that is definitely an instance where I was really happy with the result, and, uh, I think we're gonna talk about another project where I did the exact opposite, so, um, I think there's definitely benefits of, of reflecting on the visualization and adding, uh, the experts' opinion on what the charts say, uh, and there's, which we'll talk about soon, um, a big benefit of, of on purpose not doing that. And, and what that can elicit from the readers and the internet in terms of how they respond to the project. 


EB: Yeah, yeah, and another aspect, I just want to briefly mention that too, you seem to make the charts sometimes so the first chart typically is the one that tries to answer the narrow question that you started with, but then as you progress, you give more freedom to the reader to explore some aspects on his own, right? Which is also interesting. I think this is, this has been called in the past something like the martini glass structure, so I guide you through some, some data facts, and as soon as you know enough about it, then you are ready to kind of like explore it on your own a little bit.


MD: Yes, absolutely. Um, that is a thing I've done on every project, which is, uh, not starting with the whole data set. Um, if I were to start off with a chart that is just purely about the present-day popularity of older music, and I think the whole data set was tens of thousands of songs, it would just be an overwhelming visualization. Uh, people would walk into it and say, this is too complex, I don't know what it says. Uh, so I purposefully narrowed the data set just to the '90s, and I've done this on almost every project, just to get, almost like an amuse-bouche for, uh, for the article, of, like, okay, I'm picking up what you're putting down. This is an article about whether "No Diggity" is still popular today, relative to "Smells Like Teen Spirit." And that, I think, builds the mental model to then go look at 50,000 songs' play counts by year, and then also their historic Billboard data, which is, again, like, we're talking about hundreds of thousands of data points, but once they have that mental model built with that small chart, uh, it's a lot easier to process. So, you're absolutely right. That is, that is like a, a visual trick I've tried to employ on almost all the articles.


MS: Yeah, this framing, like, starting with the right question, or, like, what is the entry point to the whole thing, can totally make or break these sort of complex, uh, projects.




24:15 Our sponsor: CartoDB
  

MS: This is a good time to talk about our sponsor this week. This episode of Data Stories is sponsored by CartoDB. CartoDB is an open, powerful, and intuitive platform for discovering and predicting the key facts underlying the massive location data in our world. And recently, they announced a partner with Metson to provide location data services, which you can use either inside CartoDB, or even license them to your application. They provide custom base maps which are customized (inaudible)??? maps supported with worldwide coverage. They also offer geocoding services so you can turn plain text into location coordinates using the built-in geocoder, and you can custom geocode your data by country, county, or municipality. Choose from high-accuracy street addresses, or map your locations by any global postal code. And they also provide routing services, so based on open street maps road network data, CartoDB's routing services provide easy driving, walking, and cycling and turn-by-turn directions, and it also includes a cool feature called time and distance isolens (inaudible), so you can draw on a map how far you can actually get with 20 minutes of walking, for instance, from a given point. With CartoDB, analyzing and designing beautifully insightful maps has never been easier. Check out incredible location intelligence projects, and get started for free at CartoDB.com/gallery. And now, back to the show. 



25:49 Matt's project on The Bechdel Test
  

MS: There's another one that I would like to talk about. Actually, it's two, so it's a duo of projects, as far as I understood. So, you did a look into Hollywood's, uh, gender balance, or gender divide, maybe. And, uh, yeah, there's actually two articles on Polygraph related to that. Can you tell us a bit about how that story unfolded? 


MD: Yeah, um, I think we should talk about the latter one, but certainly talk about the first one, um, as it relates to the latter. So, the project started with, again, a question around the Bechdel test, which is, uh, for those not familiar with it, um, it started almost as a semi-joke from a comic strip, uh, written by Alison Bechdel, and another coworker. They had this comic strip that was, again, a half-joke about, uh, the lead in the comic only wanted to see movies that fit three different criteria. Uh, and the criteria are there are two women who talk to each other at some point in the movie about something other than a man, and there's some additional criteria that people have added over the years, but that's the spirit of it. So, it seems like an embarrassingly low bar, um. 


MS: Yeah, you'd think any movie easily passes, right? 


MD: Easily passes.


MS: Yeah, how couldn't there be a movie that at least once there are two women that talk about something, I mean how hard can it be? 


MD: How hard can it be? Um, well that's the joke, is that, I mean, and a sad fact of reality is a lot of movies don't pass this test. In fact, there's a site kind of like Wikipedia called bechdeltest.com, where you can go and Wikipedia-esque crowdsource whether movies pass or fail this test, so you can probably go to this site and see any movie that's in the box office today and either see its rating or add your own rating. And by rating, I mean whether it passes or fails the test, and each of the three criteria. So, uh, so I knew about this test, and had a question in my mind that the results of the test were less a function of, like, systemic sexism and, like, we just don't want two women talking to each other about men, but rather most movies are written by men, and you would expect a bunch of men in a room to not write very, uh, inclusive stories from a gender perspective. So, the question I started with was, if you took all the movies that pass or fail this test, to what extent is that a function of the gender of the writers and the producers and the directors? So, there was roughly about 5,000 films on bechdeltest.com, scraped all those films, got the results, and yeah, it was pretty obvious, like, when you had at least one woman on the writing team, the rate at which movies pass the test goes up dramatically, and when there's an all-women writing team, it's something like 95 percent of films pass this test. Um, when it's just men, it's about 55 or 60 percent pass the test, so it's pretty obvious, and who knows if it's right? Uh, I didn't run any crazy modeling correlation, I was just, like, here's the data, if you want to question the statistical significance, everything's open-source and downloadable, and you can do your own modeling, but here's just the high-level results for the layperson. And the response to that project was actually pretty embarrassing, um, from the internet, and by embarrassing I mean most…


MS: So did you meet the Reddit crowd, or what happened? 


MD: Oh, I love the Reddit crowd. I love, I live for Reddit comments. Um, yeah, it was pretty terrible. Uh, I don't want to go into depth about exactly what was said, but most people, uh, the reason why the Bechdel test exists, in many ways, is because, uh, there is a well, almost like undercurrent of, of, of what is perceived to be poor gender inclusivity in film. However, uh, people perceive that there's reasons for that, such as alright, we have a lot of war movies and historic movies. Are you gonna cast all women in "Saving Private Ryan"? Um, which is a fair point in some ways. However, however, however, that's one movie, and Hollywood puts out lots of movies, so what would often happen is we get stuck in the discourse of gender inclusivity around these, these points of, well, what do we do about, about war films, and people pay for movies, so if they're paying for all-male movies, like, do we want to change the economics, and should we censor writers? Like, should we force them to have more gender-inclusive movies? So, there's just a lot of things wrong with the current discourse, and people got hung up on, well, the Bechdel test was a very biased test, and when you look at, like, whether movies are pass or fail this test, it's, the test is so emotionally charged. Um, and not only that, it's kind of a crappy test. So, you have movies like "Jurassic World" that pass this test, but really don't do anything from, like, a gender inclusivity perspective for women.


MS: It's also very binary, you know? It's, like, either you pass or fail. I mean, as if it wasn't an exam, I mean. Yeah.


MD: So, again, I had a lot of poor response from the internet on this project, even though I think it did pretty well traffic-wise. And the traffic was mostly the echo chamber of people who already were complaining about gender inclusivity, so in terms of improving the discourse on the internet around this topic, it really didn't do anything. 


MS: Just a lot of outrage on both sides, and a lot of traffic (inaudible).


MD: Yeah, yeah. Well, I'm not trying to do (inaudible). I think the story was, like, can we improve the discourse around this topic in the same way, you know, uh, very lightly, we did that with the Timeless Music project of can you improve the discourse of why music stands the test of time. Like, why do some tracks get lost in, in time and some get only more popular?


MS: Yeah, and there's so much anecdotal info about this, you know? Everybody comes up with one example, or five examples, and I think it's so interesting to look at 5,000 and see how it plays out. And, I think, this is what you did, right?




32:00 The follow-up project: Film Dialogue from 2,000 Screenplays, Broken Down by Gender and Age
  

MD: Yeah, so the sequel to the Bechdel test project was almost a revenge project. I was so angry about the Bechdel test project.


EB: Yeah, like this time I'm gonna do it right.


MD: Yeah, seriously, that is exactly what happened. So, I recruited some friends, and, uh, well, the Bechdel test project was actually done with a woman, uh, in film. So, so, that was a collaboration, and then the second project I brought in another woman, um, who's a real engineer. Again, I just taught myself to code a year ago. And she helped out getting the data for the revenge project, which was to kill the Bechdel test as this measure for gender inclusivity and, and, and actually get better data, which we decided would be just looking at raw dialogue by gender. So, instead of saying, okay, we're gonna have this imaginary test that Alison Bechdel used as a half-joke in a comic strip 40 years ago, we're going to have a, a, another way to quantify gender inclusivity, um, using just the percent of dialogue from screenplays that are men versus women. So that's, again, not a perfect measure, but, in my opinion, way more, uh, an improvement over the Bechdel test, um, for all the reasons already discussed. So, that was the second project and the one I really want to talk about, uh, which is, uh, film dialogue broken down by gender and age. And that project came out a few months, or in April, and it did really well, and I think, uh, is also directionally more of the type of work that I'm hoping to do in the near future. 


MS: So, what did you find? Now everybody wants to know. 


MD: Yeah, so, uh, so, I encourage everyone to go look. Um, which is polygraph.cool/films. And the, the spirit of, I think, the article, is just to show the data, and, um, a lot of people emailed me and said, well, why didn't you publish a, a result, of, okay, you have, I don't even know what the number is, like, 70 percent male, 30 percent female. Um, I purposefully didn't do that. It is a visual of 2,000 films, uh, essentially a histogram of 2,000 films, uh, shaded and plotted by their percent of men, male dialogue versus women. And what you see is basically the, the balance very, very much weighted towards the male side. And you can actually look at the films, so there's no abstractions, there's no overall percent. It is hover over "Jurassic World" and actually see the number, and also the character breakdown, and as much detailed data as we could get. So this was a very, very complex project. 


MS: And you can compare by, by genre, you even looked at the actors' ages, which I found super interesting, like, is there a difference, you know, how old the different roles are. Spoiler, women are much younger in films, and so this is all very interesting. 


MD: I wanted to shut down the anecdotal, "well, what about war films?" comments, because, which are valid, again, but it just, like, it was totally anecdotal. I would be, like, well, yeah, that's fair, but most movies aren't war films, so the point was to get as quantify and visualize all the data, down to number of lines for, uh, um, Chris Evans in "Jurassic World." I think that's the actor. So, we're getting as detailed as possible, so if people want to say, well, what about X? You can go look at that data without knowing how to code and downloading all the data from GitHub. Like, every visual is to support any form of exploration that you have in your mind, down to, like, evolution of the data by decade, down to, uh, just Disney movies. Uh, so, the purpose of this project was really to almost build a terminal, or a console, or, um, a dashboard for this data that otherwise would be stuck in, in, in kind of these abstractions that you typically see in academic studies around gender inclusivity. 


EB: Yeah, I think that's an aspect that I really like. I think here you found very nice balance between making the data accessible, right? But at the same time not trying to impose any specific, um, outcome, or even a hypothesis, right? I really like, I cut this sentence from, from your text. You write, "We didn't set out trying to prove anything, but rather compile real data. We framed it as a census, rather than a study." And I really like, I really, really like that. Um, yeah, maybe you can comment a little bit more on this kind of mindset? 


MD: Yeah, well, Reddit, Reddit got really angry about that as well. They were, like, how can you publish...?


MS: Now he's trying to sneak out of his responsibility. 


MD: Yeah, yeah.


EB: It's, it's a fine line, right? Because, yeah, I don't know. I find that this is a little, a big struggle for people like you, spend a lot of time trying to analyze data and make the result of this analysis, um, digestible by people, right? And at the same time, trying to find the right balance between not imposing your own view, but not making the whole thing too hard to understand it. You have to start from scratch, right?


MD: If this project was to prove a, whether Hollywood is gender-inclusive, it would go nowhere. Because if that's the hypothesis, anyone going into this with any bias is going to be, like, well, this is total bullshit for X, Y, and Z reasons. 


MS: You can always take that apart, yeah. That's the thing, I mean, you analyze pop culture with numbers, right? And this is, I think is always interesting, and there's always super much to be learned, but it's pretty much impossible, I think, to prove anything cultural, like, just with numbers. You know? There's always, like, a shortcoming in, oh, you could also have measured this, or you're not taking into account that. Or, you know, it's, you can just show, yeah, one perspective. I think that's clear. 


MD: Yeah. Which is why you can't prove anything, um, so, so, so, someone said, well, you haven't proved...


MS: Yeah, but still, some data's better than no data. 


MD: Yeah. Someone had written a, like, 3,000-word comment on Reddit that was picking apart everything. And then the response to this comment was so beautiful. It was, like, this is, like, death by 1,000 nitpicks, or, um, this is a well-written response that ignores the overwhelmingly glaringly obvious point and the data. Which is fine, but, again, like, my point wasn't prove to anything. It was that the only data that we had around gender inclusivity was anecdotal and the Bechdel test, which is a pretty sad state when we're talking about this in, in so many forms of, of culture around the Oscars are so white in America, and the Geena Davis Institute, which is constantly trying to promote more women in director roles, so, uh, (inaudible) the point of this project was, was essentially me acting as a census, uh, as a data-gathering instrument. And for that purpose, I tried to avoid any modeling and just present the data.


EB: Yeah. Yeah, yeah. But I have to say, I think here you are doing something really important, because, I mean, when you think about what kind of, who are the figures out there who are proposing ideas based on data, traditionally, right? On the one hand, you have scientists, on the other hand, you have people like journalists, and politicians. And all of them, one way or another, have some kind of agenda, or at least an hypothesis to, to, to prove, right? Or an intent to persuade you about something. And, and, you're kind of, like, moving away from that here, and still make the data and the ideas behind this data accessible. And I find this, this format really, really interesting.


MD: That was one of the top comments from the internet, was what is your agenda on this, like, what are you trying to…


EB: You have no agenda, I guess, right? 


MD: Well, yeah, yeah. I mean, I had an agenda of, like, get the data. But my agenda wasn't to, like, overhaul Hollywood. My agenda was, we're sitting around having anecdotal discussions about a thing that is easily quantifiable, and just no one wanted to qualify it. And there's a reason why no one wanted to quantify it, because it just takes a lot of effort. And we can go into the technical side of this, but we spent six weeks just gathering data.


EB: Yeah, yeah.


MD: Um, and, and that was a labor of love, also a very stressful experience, because, as you can imagine, screenplays are one of the, is, getting dialogue from screenplays is not a structured dataset at all. Um, so, so that was a fun experience, um, but also one that I felt was really just needed and to improve and move forward the discussion that we were having around this topic.


MS: Yeah.


EB: Maybe you can briefly explain how you did, because the data collection and analysis here looks like a daunting task. Like, how did you, how did you collect this data and make sure that it had some, some okay quality at least?


MD: Yeah, um, I mean, I am my own personal bar. This is not peer-reviewed, this didn't go through, like, a New York Times editor. And, honestly, this project probably would have gotten shut down if it had gone through all those channels, but, you know, I think my bar is pretty high in terms of quality. So, that's what we worked with, is would I accept these results if I saw them on the internet. Um, and I wrote a whole FAQ about, like, what's wrong with the data, um, which we can talk a little bit about now. But in terms of gathering the data, um, the engineer I was working with, like, basically was, like, you can't do this. This is too complicated. Um, so, I (inaudible) Yeah, and there's definitely errors. I mean, there's always errors. There's more errors in (inaudible) just methodologically, we're using screenplays, which already means there's, uh, gonna be issues with, uh, how accurate that represents the film, which is obviously a product of the screenplay. So, that alone means there's tons of errors, uh, but if you accept that generally, there's not a huge, there isn't a consistent shift from the screenplay to the film, we still have directionally accurate data. Uh, but anyway, we chose to go with screenplays. Um, there's other ways to, to go about this. You could use onscreen dialogue, which means you'd have to watch the film and then categorize who's talking, and how long they're talking, and how long they're talking. You know, that's an option, but we chose to use screenplays, and the idea was, uh, could you break down the screenplays by character, and then you would have to, once you had that, so we have the lead character has 8,000 of the 12,000 words uttered in the screenplay, and then from there we have to figure out, well, what gender is this lead character? Uh, and we methodologically went with, uh, connecting the lead, the character name to an actor, uh, on IMDB, or, like the cast list. And then the actor to a gender. Um, so that was the, the, the thread, that was the thread through the needle that we needed to figure out, was screenplays to characters to actors to genders. And do that 2,000 times. And with as little error as possible. So, so that was the complexity of the project, and the most amount of complexity was just in screenplays are formatted uniquely every single time, they're not always text files, they're sometimes PDFs.


MS: Yeah, they're made for humans, not for machines, right? So (inaudible)


MD: Exactly. And if you're old enough, you know the acronym OCR, which is, uh, taking pages that are scanned and converting them to computer-readable text, and, you know, a lot of these movies are scanned PDFs from the '70s, so they're not rich text formatted, they're written in terrible fonts with, like, uh, noise in the scans, so little lines and, like, dots everywhere from whatever Xerox machine they used, and yeah, it's just like a very messy dataset. So, there was a lot of complexities getting it done, and that's why it took six weeks. 




44:19 The financials behind Matt's work
  

MS: That's, that's quite good. Yeah, I mean it sounds incredibly laborious. I mean, first of all, thanks for doing this, so now it's available, and you know people can work with that. But the other thing is also practically, like, how do you do that? So, basically, you are building this data journalism platform right now and going to these really complex data investigations, do cool graphics. How do you make that work from a financial standpoint? Do you just try to do something cool now and worry about that later, or do you get paid for the work already, or do you have a plan to get paid? Like, how do you think this will play out?


MD: Um, I don't know. Right now, it's make cool things on the internet. Um, yeah. I have, I mean, what I'm doing now was my side hustle to a full-time job, and now I'm flipping it around. It's do this full time and find side hustles. So I'm keeping the lights on, I'm still well-fed. Um, I'm trying to, again, get to that point where I have someone else working with me on this full-time potentially, and you know I'd love to grow it to the size of FiveThirtyEight. You know, making money will probably be a lot like how they make money. Um, although they're completely subsidized by ESPN, so.


MS: That's what I was going to say, (inaudible)


MD: Yeah, I don't know. This podcast has a sponsor. Who knows? (inaudible) But yeah, I haven't quite figured that out yet. Um, so I'm scraping by just trying to make as many cool things as possible. Um, and you know, the spirit of all these projects is, is take a question that, that has, has interesting, but very, like anecdotal discourse around it, and then add some data to the discussion, so the Timeless project was adding data to talk about, well, why do some songs stand the test of time and this film project was adding data to how do we even think about gender inclusivity and why there's imbalance in certain areas versus others. And then future projects will, will try to move that even farther. And I do hope that one day, I figure out how to pay for this and get sponsors, but for now, it's just an exercise in data gathering and talking about culture in ways that I don't think has been done quite the way that Polygraph has approached it. 


EB: Oh, yeah. I mean, Matt, I wish you all the best, because the work you are doing is, is honestly amazing, and, uh, it really shows it's clearly a labor of love. Every single detail in your work is just shows how much effort there is behind that, and, uh, how much care. You can see how deeply you care about everything. It goes from the data analysis, data collection, the visuals, the narrative, everything is fantastic. Congratulations. 


MD: Thank you.


EB: Seriously, I mean, I even say that lightly. I'm really impressed by your work. It's fun and it's inspiring at the same time. 




47:20 Stay tuned on Polygraph: Matt's next project
  

MD: Yeah. I'm trying to, I mean, the next projects, I think, I'm even more excited about. Um, I've done a lot of music projects, and I'm slowly inching away from doing more serious topics that, again, have the same spirit of adding a little bit more definition and data around something that's pretty amorphous in culture. Uh, so, thank you and it's only going to get better, hopefully. I'm getting better at coding every day, so that's good. 


MS: Yeah, we can't wait to see your next projects. Let us know when they come out.


EB: Yeah, absolutely. Absolutely. 


MD: Yeah, well, a teaser: the next one is going to be on slavery in the U.S., which is the most, yeah, that's, I'm moving from, like, "No Diggity" and '90s music to slavery. A logical progression, yeah, but it's honestly, I think, a good challenge, because I'm taking a topic that has no, very little intrinsic interest publicly and trying to make it, um, something that people will really lean into. So, I'm taking it as a challenge of, can I apply what I've done to music and film and a couple of other topics, mostly pop culture-related, to something that is very, uh, nerdy, more nerdy than the things I've done in the past. So, uh, it's a good experiment, and I think it'll be pretty worthwhile.


EB: Yeah, I think yesterday, sifting through your website, I found (inaudible) somewhere, where you are imitating keeping track of ideas, and it was like, oh yes, do this, this, this as well. Please, do it. 




48:45 Interested in collaborating with Matt? Get in touch
  

EB: So, if there is anyone who aspires to work with you, what should this person do?


MD: Oh, like, from a collaboration perspective? Yeah, I mean, it's a very depressing state right now because I can only move as fast as I can work, and I've been very, very aggressively trying to find ways to move faster involving more people. Um, so I absolutely encourage anyone to reach out to me at matt@polygraph.cool from an email perspective. Um, but really, I need people who can essentially do the type of work that's on Polygraph, which is heavy data visualization, writing, design, like, really the soup to nuts writing articles. Um, there's a lot of people who can do some of those things very well, like a developer, or a designer, or a writer. But what I've found is the best work is really people who can wear all those hats. So if you absolutely, if you're one of those people who have three of those hats on at the same time, um, you're probably a unicorn and we should work together.


EB: Great. Uh, well, okay, thanks, thanks a lot for coming on the show. I mean, we could go on forever. And, um, we are really looking forward to seeing what you are, what you publish next. And, uh, I wish you the best of luck. Thanks for coming on the show.


MD: Thank you. Much appreciated. 


MS: Thanks, Matt. Bye-bye.


EB: Bye. Bye-bye.




50:23 Rate us and get in touch!
  

EB: Hey, guys. Thanks for listening to Data Stories again. Before you leave, we have a request. If you can spend a couple of minutes rating us on iTunes, that would be extremely helpful for the show.


MS: And here's also some information on the many ways that you can get news directly from us. We're of course on Twitter at twitter.com/datastories, we have a Facebook page at Facebook.com/datastoriespodcast all in one word, and we also have an email newsletter. So if you want to get news directly into your inbox and be notified whenever we publish an episode, you can go to our homepage, datastori.es and look for the link that you find on the bottom, in the footer.


EB: So, one last thing that we want to tell you is that we love to get in touch with our listeners, especially if you want to suggest a way to improve the show or amazing people you want us to invite, or even projects you want to ask to talk about.


MS: Yeah, absolutely, so don't hesitate to get in touch with us. It's always a great thing for us. And that's all for now. See you next time, and thanks for listening to Data Stories.




51:24 Our sponsor: CartoDB
  

EB: This episode is sponsored by CartoDB. CartoDB is an open, powerful, and intuitive platform for discovering and predicting the key facts underlying the massive location data in our world. With CartoDB, analyzing and designing beautifully insightful maps has never been easier. Check out incredible location intelligence projects and get started for free at cartodb.com/gallery. That's c-a-r-t-o-d-b dot com, slash gallery.